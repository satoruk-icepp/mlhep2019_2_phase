{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_lhcb_calo_gan_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satoruk-icepp/mlhep2019_2_phase/blob/master/Bayesian_lhcb_calo_gan_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giyM_o8bmDLI",
        "colab_type": "code",
        "outputId": "db51d238-065a-46b2-fa2f-743a2eba117c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud8PAMb2mDK9",
        "colab_type": "code",
        "outputId": "5fe15d27-4375-45be-ba77-203d884faa94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "! wget https://raw.githubusercontent.com/SchattenGenie/mlhep2019_2_phase/master/analysis/calogan_metrics.py -nc\n",
        "! wget https://raw.githubusercontent.com/SchattenGenie/mlhep2019_2_phase/master/analysis/prd_score.py -nc\n",
        "! wget https://raw.githubusercontent.com/SchattenGenie/mlhep2019_2_phase/master/analysis/score.py -nc\n",
        "! wget https://github.com/satoruk-icepp/mlhep2019_2_phase/raw/master/analysis/embedder.tp -nc\n",
        "! wget https://github.com/satoruk-icepp/mlhep2019_2_phase/raw/master/Regressor.py -nc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘calogan_metrics.py’ already there; not retrieving.\n",
            "\n",
            "File ‘prd_score.py’ already there; not retrieving.\n",
            "\n",
            "File ‘score.py’ already there; not retrieving.\n",
            "\n",
            "File ‘embedder.tp’ already there; not retrieving.\n",
            "\n",
            "File ‘Regressor.py’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXxnaK7TH6vO",
        "colab_type": "code",
        "outputId": "3b5aa31c-ce16-4ca2-fb48-4397d9b1101f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile .comet.config\n",
        "[comet]\n",
        "api_key=mIel5ZAPOioTs0Cij75dSSQXs\n",
        "logging_file = /tmp/comet.log\n",
        "logging_file_level = info\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting .comet.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlDzipgxH9kV",
        "colab_type": "code",
        "outputId": "7b219a7a-8991-41c1-d5b1-7941d5464a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install skorch comet_ml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.1)\n",
            "Requirement already satisfied: comet-git-pure>=0.19.11 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.19.14)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.21.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.2)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.57.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.8)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLgVy1i9zZiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE         = 75\n",
        "N_EPOCH            = 200\n",
        "N=50250\n",
        "NOISEIMAGE_DIM = 5\n",
        "NOISE_DIM = NOISEIMAGE_DIM**2\n",
        "CONDITION_DIM=6\n",
        "IMAGE_DIM=30\n",
        "unrolled_steps= 0\n",
        "Nredconv_gen = 10\n",
        "lambda_reg =10\n",
        "use_additionalinfo = False\n",
        "restartfrom=0\n",
        "params={\n",
        "    'Nepoch':1000,\n",
        "    'batch_size' : 75,\n",
        "    'train_size'  : 3000,\n",
        "    'val_size'  : 300,\n",
        "    'noise_dim' : NOISE_DIM,\n",
        "    'EScale':4000,\n",
        "    'PScale':[30,30,100],\n",
        "    'PosScale':[10,10],\n",
        "    'PDGScale':11,\n",
        "    'Nresblock':0,\n",
        "    'dropout_disc':0.0,\n",
        "    'dropout_gen':0.0,\n",
        "    'ntrain_d':5,\n",
        "    'Ncond_dim':5,\n",
        "    'gp':True,\n",
        "    # 'task':\"HINGE\",\n",
        "    # 'task':\"NORMAL\",\n",
        "    'task':\"WASSERSTEIN\",\n",
        "    'optim':'Adam',\n",
        "    # 'optim':'RMSProp',\n",
        "    'weight_decay':1e-5,\n",
        "    'noiselevel':0.001,\n",
        "    'learning_rate':5e-4,\n",
        "    'lrratio':1,\n",
        "    'LRgamma':1,\n",
        "    'LRtype':\"Step\",\n",
        "    'stepsize_lr':100,\n",
        "    'base_lr':1e-06,\n",
        "    'AUX':False,\n",
        "    'conditional':True,\n",
        "    'test_sample':False,\n",
        "    'alpha':0.1\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CjhrWKT4Ig",
        "colab_type": "code",
        "outputId": "6f43aaa6-3e0f-41b6-966e-492131c22d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from comet_ml import Experiment\n",
        "experiment = Experiment(project_name=\"BayesGAN\")\n",
        "experiment.log_parameters(params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/satoruk-icepp/bayesgan/0320c901c8d440e7937887a65cf4be43\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLY8nIkZLn24",
        "colab_type": "code",
        "outputId": "a21c6b0d-2d0b-464c-f751-469ed6a8ce36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from skorch import NeuralNetClassifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "790_h5rSmDLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/PySource/')\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "sns.set()\n",
        "\n",
        "def one_hot(a, num_classes):\n",
        "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxdV1KUd4IFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from NetworkUtil import ResidualBlock,ReducedConv2,ReducedConv,Self_Attn,normal_init,init_weights\n",
        "from CosineExp import CosineExpLR\n",
        "from torch.nn.utils import spectral_norm\n",
        "from Test_Image import Make_Sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW0lyRVGmDLC",
        "colab_type": "code",
        "outputId": "829333e9-a4a7-44ec-9283-a79dec5bf87b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJbcY6IwmDLK",
        "colab_type": "text"
      },
      "source": [
        "## Data pathes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5muy7MJmDLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_path = '/content/drive/My Drive/mlhep2019_gan/data_train.npz'\n",
        "# train_data_path = '/gdrive/My Drive/data_afteraugmentation.npz'\n",
        "val_data_path = '/content/drive/My Drive/mlhep2019_gan/data_val.npz'\n",
        "test_data_path = '/content/drive/My Drive/mlhep2019_gan/data_test.npz'\n",
        "\n",
        "# train_data_path = '../data_train.npz'\n",
        "# val_data_path = '../data_val.npz'\n",
        "# test_data_path = '../data_test.npz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfXOt6RZgnTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def GetDiscPath(epoch):\n",
        "#     return '/gdrive/My Drive/disc_%d.pt'%(epoch)\n",
        "# def GetGenPath(epoch):\n",
        "#     return '/gdrive/My Drive/gen_%d.pt'%(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqklR3OHmDLM",
        "colab_type": "text"
      },
      "source": [
        "# Loading data\n",
        "\n",
        "Data is stored in `.npz`-format which is a special filetype for persisting multiple NumPy arrays on disk. \n",
        "\n",
        "More info: https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.format.html#module-numpy.lib.format.\n",
        "\n",
        "File `dat_train.npz` contains four arrays: \n",
        "\n",
        "  * `EnergyDeposit` - images of calorimeters responses\n",
        "  * `ParticleMomentum` - $p_x, p_y, p_z$ of initial partice\n",
        "  * `ParticlePoint` - $x, y$ of initial particle\n",
        "  * `ParticlePDG` - particle type(either $e^-$ or $\\gamma$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkvMrGHqj_S0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EnergyDepositScale = torch.tensor([EnergyDepositScale]).float().to(device)\n",
        "# EnergyDepositOffset = torch.tensor([EnergyDepositOffset]).float()\n",
        "# MomentumScale      = torch.tensor([PXscale,PYscale,PZscale]).float()\n",
        "# MomentumOffset      = torch.tensor([0,0,PZoffset]).float()\n",
        "# PointScale         = torch.tensor([XPosscale,YPosscale]).float()\n",
        "# PointOffset        = torch.tensor([0,0]).float()\n",
        "# ParticlePDGScale   = torch.tensor([PDGscale]).float()\n",
        "# ParticlePDGOffset   = torch.tensor([PDGoffset]).float()\n",
        "# MomentumPointPDGScale = torch.cat([MomentumScale,PointScale,ParticlePDGScale]).to(device)\n",
        "# MomentumPointPDGOffset = torch.cat([MomentumOffset,PointOffset,ParticlePDGOffset]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcLIRNvMmDLN",
        "colab_type": "code",
        "outputId": "5e697ead-8e04-4eb1-d2c4-a8b6c9ae8240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# N = 1000\n",
        "\n",
        "data_train = np.load(train_data_path, allow_pickle=True)\n",
        "print(list(data_train.keys()))\n",
        "N = len(data_train['EnergyDeposit'])\n",
        "print(N)\n",
        "# N=1500\n",
        "# [data_size, 900]\n",
        "EnergyDeposit = data_train['EnergyDeposit'][:N]\n",
        "# reshaping it as [data_size, channels, img_size_x, img_size_y]\n",
        "# channels are needed for pytorch conv2d-layers\n",
        "EnergyDeposit = EnergyDeposit.reshape(-1,1,30,30)\n",
        "EnergyDeposit = EnergyDeposit/params[\"EScale\"]\n",
        "\n",
        "# [data_size, 3]\n",
        "# ParticleMomentumScale = [PXscale,PYscale,PZscale]\n",
        "ParticleMomentum = data_train['ParticleMomentum'][:N]\n",
        "ParticleMomentum = np.divide(ParticleMomentum,params[\"PScale\"]) \n",
        "\n",
        "# [data_size, 2]\n",
        "# ParticlePointScale = [XPosscale,YPosscale]\n",
        "ParticlePoint = data_train['ParticlePoint'][:, :2][:N]\n",
        "ParticlePoint = np.divide(ParticlePoint,params[\"PosScale\"])\n",
        "\n",
        "# [data_size, 1]\n",
        "ParticlePDG = data_train['ParticlePDG'][:N]\n",
        "ParticlePDG = np.divide(ParticlePDG-1.5*params[\"PDGScale\"],params[\"PDGScale\"])\n",
        "ParticlePDG = ParticlePDG.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['EnergyDeposit', 'ParticlePoint', 'ParticleMomentum', 'ParticlePDG']\n",
            "50250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK6ep9kSEvCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.hist(ParticlePDG)\n",
        "# plt.show()\n",
        "# plt.hist(ParticlePoint[:,0])\n",
        "# plt.show()\n",
        "# plt.hist(ParticlePoint[:,1])\n",
        "# plt.show()\n",
        "# plt.hist(ParticleMomentum[:,0])\n",
        "# plt.show()\n",
        "# plt.hist(ParticleMomentum[:,1])\n",
        "# plt.show()\n",
        "# plt.hist(ParticleMomentum[:,2])\n",
        "# plt.show()\n",
        "# plt.hist(EnergyDeposit.reshape(-1,1))\n",
        "# plt.yscale('log')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2VP0Rt3G44V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EnergyDeposit    = torch.tensor(EnergyDeposit).float()\n",
        "# EnergyDeposit = EnergyDeposit/EnergyDepositScale\n",
        "ParticleMomentum = torch.tensor(ParticleMomentum).float()\n",
        "ParticlePoint    = torch.tensor(ParticlePoint).float()\n",
        "ParticlePDG      = torch.tensor(ParticlePDG).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAROHySmDLP",
        "colab_type": "text"
      },
      "source": [
        "## Load it to pytorch `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtEr15sdmDLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# validation_split = 0.05\n",
        "random_seed= 42\n",
        "split = params[\"val_size\"]\n",
        "indices = list(range(params[\"train_size\"]+params[\"val_size\"]))\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "calo_dataset    = utils.TensorDataset(EnergyDeposit, ParticleMomentum, ParticlePoint, ParticlePDG)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(calo_dataset, batch_size=params[\"batch_size\"], \n",
        "                                           sampler=train_sampler, pin_memory=True)\n",
        "validation_loader = torch.utils.data.DataLoader(calo_dataset, batch_size=params[\"batch_size\"],\n",
        "                                                sampler=valid_sampler, pin_memory=True)\n",
        "\n",
        "\n",
        "# calo_dataloader = torch.utils.data.DataLoader(calo_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crC_kDGkmDLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b, ParticlePDG_b in validation_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inm6pj7DmDLU",
        "colab_type": "text"
      },
      "source": [
        "## Training GAN\n",
        "###### ...is not a simple matter\n",
        "\n",
        "It depends on architecture, loss, instance noise, augmentation and even luck(recommend to take a look https://arxiv.org/pdf/1801.04406.pdf)\n",
        "\n",
        "\n",
        "In this notebook I have prepared some basic parts that you could use for your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNKTOTNkmDLb",
        "colab_type": "text"
      },
      "source": [
        "#### Small hack that can speed-up training and improve generalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_qOnCr6Zias",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from bayes import NoiseLoss,PriorLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ttB8J3UqR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_instance_noise(data, std=0.01):\n",
        "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT8YWKN5mDLg",
        "colab_type": "text"
      },
      "source": [
        "## Defining discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4_WffK-mDLi",
        "colab_type": "text"
      },
      "source": [
        "## Defining generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD3j9qwIe9MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import Label2Image\n",
        "# from generator import ModelGConvTranspose\n",
        "# from discriminator import ModelD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmuKR7PsRuID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from NetworkUtil import ResidualBlock,ReducedConv2,ReducedConv,Self_Attn,normal_init,init_weights\n",
        "from CosineExp import CosineExpLR\n",
        "from torch.nn.utils import spectral_norm\n",
        "from Test_Image import Make_Sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkbHG6fDNGKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Generator(nn.Module):\n",
        "#     def __init__(self,dropout_gen=0., Ndim_cond=0):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.Ndim_cond = Ndim_cond\n",
        "#         self.fc1 = nn.Linear(NOISEIMAGE_DIM**2+self.Ndim_cond,256)\n",
        "#         self.fc2 = nn.Linear(self.fc1.out_features,self.fc1.out_features*2)\n",
        "#         self.fc3 = nn.Linear(self.fc2.out_features,16*64)\n",
        "#         # self.deconv0 = ReducedConv2(NOISEIMAGE_DIM**2+self.Ndim_cond,64,4,3)#(1,1)->(4,4)\n",
        "#         # self.deconv0 = spectral_norm(nn.ConvTranspose2d(NOISEIMAGE_DIM**2+self.Ndim_cond,64,4))#(1,1)->(6,6)\n",
        "#         self.deconv1 = ReducedConv2(64,32,2,3)#4->8\n",
        "#         self.deconv2 = ReducedConv2(32,16,2,3)#8->16\n",
        "#         self.deconv3 = ReducedConv2(16,1,2,5)#16->30\n",
        "        \n",
        "#         # self.deconv1 = spectral_norm(nn.ConvTranspose2d(64,32,4,stride=3))#(6,6)->(12,12)\n",
        "#         # self.deconv2 = spectral_norm(nn.ConvTranspose2d(32,16,4,stride=2))#(8,8)->(16,16)\n",
        "#         # self.deconv3 = spectral_norm(nn.ConvTranspose2d(16,1,3))#(16,16)->(30,30)\n",
        "#         # self.bn0=nn.BatchNorm2d(self.deconv0.out_channels)\n",
        "#         self.bn1=nn.BatchNorm2d(self.deconv1.out_channels)\n",
        "#         self.bn2=nn.BatchNorm2d(self.deconv2.out_channels)\n",
        "#         self.ssconv2=nn.Conv2d(self.deconv2.out_channels,self.deconv2.out_channels,3,padding=1)\n",
        "#         self.rb1 = ResidualBlock(self.deconv1.out_channels)\n",
        "#         self.rb2 = ResidualBlock(self.deconv2.out_channels)\n",
        "#         self.dropout = nn.Dropout(dropout_gen)\n",
        "#         # self.activation  = nn.LeakyReLU(alpha)\n",
        "#         self.activation  = nn.ReLU()\n",
        "\n",
        "#     def forward(self,x,condition=None):\n",
        "#         x = x.view(x.shape[0],-1)\n",
        "#         if condition is not None and self.Ndim_cond>0:\n",
        "#             # condition = (condition-MomentumPointPDGOffset)/MomentumPointPDGScale\n",
        "#             x = torch.cat([x,condition],dim=1)\n",
        "#         x = self.activation(self.fc1(x))\n",
        "#         x = self.activation(self.fc2(x))\n",
        "#         x = self.activation(self.fc3(x))\n",
        "#         # x = x.view(x.shape[0],-1,1,1)\n",
        "#         x = x.view(x.shape[0],64,4,4)\n",
        "#         # x = self.activation(self.dropout(self.bn0(self.deconv0(x))))\n",
        "#         x = self.activation(self.dropout(self.bn1(self.deconv1(x))))\n",
        "#         # for i in range(5):\n",
        "#         #     x = self.rb1(x)\n",
        "#         x = self.activation(self.dropout(self.bn2(self.deconv2(x))))\n",
        "#         # x = self.deconv2(x)\n",
        "#         # for i in range(5):\n",
        "#         #     x = self.activation(self.dropout(self.bn2(self.ssconv2(x))))\n",
        "#         x = self.deconv3(x)\n",
        "#         return torch.tanh(x)\n",
        "#         # return x\n",
        "#         # return torch.max(x, torch.zeros(x.size()).to(device))\n",
        "#         # return torch.sigmoid(x)\n",
        "\n",
        "#     def weight_init(self, mean, std):\n",
        "#         for m in self._modules:\n",
        "#             normal_init(self._modules[m], mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqNhoIYZL3yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self,dropout_gen=0., Ndim_cond=0):\n",
        "        super(Generator, self).__init__()\n",
        "        self.Ndim_cond = Ndim_cond\n",
        "        self.fc1 = nn.Linear(NOISEIMAGE_DIM**2+self.Ndim_cond,256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features,self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features,16*64)\n",
        "        # self.deconv0 = ReducedConv2(NOISEIMAGE_DIM**2+self.Ndim_cond,64,4,3)#(1,1)->(4,4)\n",
        "        # self.deconv0 = spectral_norm(nn.ConvTranspose2d(NOISEIMAGE_DIM**2+self.Ndim_cond,64,4))#(1,1)->(6,6)\n",
        "        self.deconv1 = ReducedConv2(64,32,2,3)#4->8\n",
        "        self.deconv2 = ReducedConv2(32,16,2,3)#8->16\n",
        "        self.deconv3 = ReducedConv2(16,1,2,5)#16->30\n",
        "        \n",
        "        # self.deconv1 = spectral_norm(nn.ConvTranspose2d(64,32,4,stride=3))#(6,6)->(12,12)\n",
        "        # self.deconv2 = spectral_norm(nn.ConvTranspose2d(32,16,4,stride=2))#(8,8)->(16,16)\n",
        "        # self.deconv3 = spectral_norm(nn.ConvTranspose2d(16,1,3))#(16,16)->(30,30)\n",
        "        # self.bn0=nn.BatchNorm2d(self.deconv0.out_channels)\n",
        "        self.bn1=nn.BatchNorm2d(self.deconv1.out_channels)\n",
        "        self.bn2=nn.BatchNorm2d(self.deconv2.out_channels)\n",
        "        self.ssconv2=nn.Conv2d(self.deconv2.out_channels,self.deconv2.out_channels,3,padding=1)\n",
        "        self.rb1 = ResidualBlock(self.deconv1.out_channels)\n",
        "        self.rb2 = ResidualBlock(self.deconv2.out_channels)\n",
        "        self.dropout = nn.Dropout(dropout_gen)\n",
        "        # self.activation  = nn.LeakyReLU(alpha)\n",
        "        self.activation  = nn.ReLU()\n",
        "\n",
        "    def forward(self,x,condition=None):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        if condition is not None and self.Ndim_cond>0:\n",
        "            # condition = (condition-MomentumPointPDGOffset)/MomentumPointPDGScale\n",
        "            x = torch.cat([x,condition],dim=1)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = x.view(x.shape[0],64,4,4)\n",
        "        x = self.activation(self.dropout(self.bn1(self.deconv1(x))))\n",
        "        x = self.activation(self.dropout(self.bn2(self.deconv2(x))))\n",
        "        x = self.deconv3(x)\n",
        "        return torch.tanh(x)\n",
        "        # return x\n",
        "        # return torch.max(x, torch.zeros(x.size()).to(device))\n",
        "        # return torch.sigmoid(x)\n",
        "\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdausDI9Ogjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Discriminator(nn.Module):\n",
        "#     def __init__(self,dropout_conv=0.0, Ndim_cond=0,aux=False,alpha=0.):\n",
        "#         super(Discriminator, self).__init__()\n",
        "#         # self.rconv1 = ReducedConv(1,Nlayer,)\n",
        "#         self.Ndim_cond = Ndim_cond\n",
        "#         self.conv1 = nn.Conv2d(1,16,kernel_size=2,padding=1,stride=2)#30->16\n",
        "#         self.conv2 = nn.Conv2d(self.conv1.out_channels, self.conv1.out_channels*2, kernel_size=2,stride=2)#16->8\n",
        "#         self.conv3 = nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels*2, kernel_size=2,stride=2)#8->4\n",
        "#         # self.conv4 = nn.Conv2d(self.conv3.out_channels, 1, kernel_size=4)#8->4\n",
        "#         self.conv1 = spectral_norm(self.conv1)\n",
        "#         self.conv2 = spectral_norm(self.conv2)\n",
        "#         self.conv3 = spectral_norm(self.conv3)\n",
        "#         self.rb1 = ResidualBlock(self.conv1.out_channels,False,True)\n",
        "#         self.rb2 = ResidualBlock(self.conv2.out_channels,False,True)\n",
        "#         self.ssconv1 = nn.Conv2d(self.conv1.out_channels, self.conv1.out_channels, kernel_size=3,padding = 1)#16->8\n",
        "#         self.ssconv2 = nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=3,padding = 1)#16->8\n",
        "#         self.ssconv1 = spectral_norm(self.ssconv1)\n",
        "#         self.ssconv2 = spectral_norm(self.ssconv2)\n",
        "#         self.fcstart = spectral_norm(nn.Linear(self.conv3.out_channels*16+self.Ndim_cond,256))\n",
        "#         self.fcmid   = spectral_norm(nn.Linear(self.fcstart.out_features,self.fcstart.out_features//2))\n",
        "#         self.fcend   = spectral_norm(nn.Linear(self.fcmid.out_features,1))\n",
        "#         self.fcaux   = nn.Linear(self.fcmid.out_features,self.Ndim_cond)\n",
        "#         if self.Ndim_cond>0:\n",
        "#             self.fcaux = spectral_norm(self.fcaux)\n",
        "        \n",
        "\n",
        "#         self.dropout1 = nn.Dropout(dropout_conv)\n",
        "#         self.activation  = nn.LeakyReLU(alpha)\n",
        "#         self.attn1 = Self_Attn(self.conv1.out_channels, 'relu')\n",
        "#         self.attn2 = Self_Attn(self.conv2.out_channels, 'relu')\n",
        "#         # self.dropoutfc = nn.Dropout(dropout_fc)\n",
        "#         # self.Nresblock = Nresblock\n",
        "#         # self.Nsd = Nsd\n",
        "        \n",
        "#     def forward(self, x,condition=None):\n",
        "#         # x_mppc = x/EnergyDepositScale\n",
        "#         x_mppc = x.view(x.shape[0],1,30,30)\n",
        "#         x_mppc = self.activation(self.conv1(x_mppc))\n",
        "#         # for i in range(5):\n",
        "#         #     x_mppc = self.rb1(x_mppc)\n",
        "#         x_mppc, amap_1 = self.attn1(x_mppc)\n",
        "#         x_mppc = self.activation(self.dropout1(self.conv2(x_mppc)))\n",
        "#         x_mppc, amap_2 = self.attn2(x_mppc)\n",
        "#         x_mppc = self.activation(self.dropout1(self.conv3(x_mppc)))\n",
        "#         x = x_mppc.view(x_mppc.shape[0],-1)\n",
        "#         if condition is not None:\n",
        "#             # condition = (condition-MomentumPointPDGOffset)/MomentumPointPDGScale\n",
        "#             x = torch.cat([x,condition],dim=1)\n",
        "#         x = self.activation(self.fcstart(x))\n",
        "#         x = self.activation(self.fcmid(x))\n",
        "#         x_out = self.fcend(x)\n",
        "#         x_aux = self.fcaux(x)\n",
        "#         return x_out,x_aux\n",
        "#     def weight_init(self, mean, std):\n",
        "#         for m in self._modules:\n",
        "#             normal_init(self._modules[m], mean, std)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqb2AU4pLWNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,dropout_conv=0.0, Ndim_cond=0,aux=False,alpha=0.):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # self.rconv1 = ReducedConv(1,Nlayer,)\n",
        "        self.Ndim_cond = Ndim_cond\n",
        "        self.conv1 = nn.Conv2d(1,16,kernel_size=2,padding=1,stride=2)#30->16\n",
        "        self.conv2 = nn.Conv2d(self.conv1.out_channels, self.conv1.out_channels*2, kernel_size=2,stride=2)#16->8\n",
        "        self.conv3 = nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels*2, kernel_size=2,stride=2)#8->4\n",
        "        # self.conv4 = nn.Conv2d(self.conv3.out_channels, 1, kernel_size=4)#8->4\n",
        "        # self.conv1 = spectral_norm(self.conv1)\n",
        "        # self.conv2 = spectral_norm(self.conv2)\n",
        "        # self.conv3 = spectral_norm(self.conv3)\n",
        "        self.rb1 = ResidualBlock(self.conv1.out_channels,False,True)\n",
        "        self.rb2 = ResidualBlock(self.conv2.out_channels,False,True)\n",
        "        self.ssconv1 = nn.Conv2d(self.conv1.out_channels, self.conv1.out_channels, kernel_size=3,padding = 1)#16->8\n",
        "        self.ssconv2 = nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=3,padding = 1)#16->8\n",
        "        self.ssconv1 = spectral_norm(self.ssconv1)\n",
        "        self.ssconv2 = spectral_norm(self.ssconv2)\n",
        "        self.fcstart = nn.Linear(self.conv3.out_channels*16+self.Ndim_cond,256)\n",
        "        self.fcmid   = nn.Linear(self.fcstart.out_features,self.fcstart.out_features//2)\n",
        "        self.fcend   = nn.Linear(self.fcmid.out_features,1)\n",
        "        self.fcaux   = nn.Linear(self.fcmid.out_features,self.Ndim_cond)\n",
        "        # if self.Ndim_cond>0:\n",
        "        #     self.fcaux = spectral_norm(self.fcaux)\n",
        "        \n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_conv)\n",
        "        self.activation  = nn.LeakyReLU(alpha)\n",
        "        self.attn1 = Self_Attn(self.conv1.out_channels, 'relu')\n",
        "        self.attn2 = Self_Attn(self.conv2.out_channels, 'relu')\n",
        "        # self.dropoutfc = nn.Dropout(dropout_fc)\n",
        "        # self.Nresblock = Nresblock\n",
        "        # self.Nsd = Nsd\n",
        "        \n",
        "    def forward(self, x,condition=None):\n",
        "        # x_mppc = x/EnergyDepositScale\n",
        "        x_mppc = x.view(x.shape[0],1,30,30)\n",
        "        x_mppc = self.activation(self.conv1(x_mppc))\n",
        "        x_mppc, amap_1 = self.attn1(x_mppc)\n",
        "        x_mppc = self.activation(self.dropout1(self.conv2(x_mppc)))\n",
        "        x_mppc, amap_2 = self.attn2(x_mppc)\n",
        "        x_mppc = self.activation(self.dropout1(self.conv3(x_mppc)))\n",
        "        x = x_mppc.view(x_mppc.shape[0],-1)\n",
        "        if condition is not None:\n",
        "            x = torch.cat([x,condition],dim=1)\n",
        "        x = self.activation(self.fcstart(x))\n",
        "        x = self.activation(self.fcmid(x))\n",
        "        x_out = self.fcend(x)\n",
        "        x_aux = self.fcaux(x)\n",
        "        return x_out,x_aux\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys1Z545TmDLl",
        "colab_type": "text"
      },
      "source": [
        "## Check our models on one batch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe748y6nmDLl",
        "colab_type": "code",
        "outputId": "f9facbf4-d7d7-4dce-b573-25f6245faffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "\n",
        "gens = []\n",
        "discs = []\n",
        "if params[\"conditional\"]:\n",
        "    gen  = Generator(params[\"dropout_gen\"],params[\"Ncond_dim\"]).to(device)\n",
        "    disc = Discriminator(params[\"dropout_disc\"],params[\"Ncond_dim\"],params[\"AUX\"],params[\"alpha\"]).to(device)\n",
        "else:\n",
        "    gen  = Generator(params[\"dropout_gen\"],0).to(device)\n",
        "    disc = Discriminator(params[\"dropout_disc\"],0,params[\"AUX\"],params[\"alpha\"]).to(device)\n",
        "\n",
        "gen.apply(init_weights)\n",
        "disc.apply(init_weights)\n",
        "# gen.weight_init(mean=0.0, std=0.02)\n",
        "# disc.weight_init(mean=0.0, std=0.02)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (rb1): ResidualBlock(\n",
              "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (activation): LeakyReLU(negative_slope=0.0)\n",
              "  )\n",
              "  (rb2): ResidualBlock(\n",
              "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (activation): LeakyReLU(negative_slope=0.0)\n",
              "  )\n",
              "  (ssconv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (ssconv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fcstart): Linear(in_features=1029, out_features=256, bias=True)\n",
              "  (fcmid): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fcend): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (fcaux): Linear(in_features=128, out_features=5, bias=True)\n",
              "  (dropout1): Dropout(p=0.0, inplace=False)\n",
              "  (activation): LeakyReLU(negative_slope=0.1)\n",
              "  (attn1): Self_Attn(\n",
              "    (query_conv): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (key_conv): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (value_conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (softmax): Softmax(dim=-1)\n",
              "  )\n",
              "  (attn2): Self_Attn(\n",
              "    (query_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (key_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (value_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (softmax): Softmax(dim=-1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtVI0PXgqFbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def LoadModels(epoch):\n",
        "#     gen.load_state_dict(torch.load(GetGenPath(epoch)))\n",
        "#     disc.load_state_dict(torch.load(GetDiscPath(epoch)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q-AMNR6tJpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if restartfrom>0:\n",
        "#     LoadModels(restartfrom)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUk-rAs1mDLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b, ParticlePDG_b = EnergyDeposit_b.to(device), \\\n",
        "                                                       ParticleMomentum_b.to(device), \\\n",
        "                                                       ParticlePoint_b.to(device),\\\n",
        "                                                       ParticlePDG_b.to(device)\n",
        "# print(ParticlePoint_b.shape,ParticlePDG_b.shape,)\n",
        "# ParticlePDG_b= ParticlePDG_b.view(-1,1)\n",
        "ParticleMomentum_ParticlePoint_ParticlePDG_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device),ParticlePDG_b.to(device)], dim=1)\n",
        "ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcrCO7wRmDLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# disc(EnergyDeposit_b, ParticleMomentum_ParticlePoint_ParticlePDG_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxagWRP6mDLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = torch.randn(len(EnergyDeposit_b), NOISEIMAGE_DIM, NOISEIMAGE_DIM).to(device)\n",
        "# gen(noise, ParticleMomentum_ParticlePoint_ParticlePDG_b).shape\n",
        "# gen(noise, ParticleMomentum_ParticlePoint_b).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgnGKjGF4MPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.L1Loss().to(device)\n",
        "# reg_loss = torch.nn.SmoothL1Loss().to(device)\n",
        "# criterion = nn.BCELoss()\n",
        "# gprior_criterion = PriorLoss(prior_std=1., observed=N)\n",
        "# gnoise_criterion = NoiseLoss(params=gen.parameters(), noise_std=math.sqrt(2 * gnoise_alpha * learning_rate), observed=N)\n",
        "# dprior_criterion = PriorLoss(prior_std=1., observed=N)\n",
        "# dnoise_criterion = NoiseLoss(params=disc.parameters(), noise_std=math.sqrt(2 * dnoise_alpha * learning_rate), observed=N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F1hMWmNIMz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_gradient_penalty(discriminator, data_gen, Condition_b, inp_data, lambda_reg = 10):\n",
        "    alpha = torch.rand(inp_data.shape[0], 1).to(device)\n",
        "    dims_to_add = len(inp_data.size()) - 2\n",
        "    for i in range(dims_to_add):\n",
        "        alpha = alpha.unsqueeze(-1)\n",
        "    # alpha = alpha.expand(inp_data.size())\n",
        "\n",
        "    interpolates = (alpha * inp_data + ((1 - alpha) * data_gen)).to(device)\n",
        "\n",
        "    interpolates.requires_grad = True\n",
        "\n",
        "    disc_interpolates,_ = discriminator(interpolates, Condition_b)\n",
        "\n",
        "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
        "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_reg\n",
        "    return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCaPksu4mDLz",
        "colab_type": "text"
      },
      "source": [
        "## Defining optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXVckXSZmDL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if params[\"optim\"] == \"Adam\":\n",
        "    gen_opt  = optim.Adam(gen.parameters() , lr=params[\"learning_rate\"], betas=(0.9, 0.999), weight_decay=params[\"weight_decay\"])\n",
        "    disc_opt = optim.Adam(disc.parameters(), lr=params[\"learning_rate\"]*params[\"lrratio\"], betas=(0.9, 0.999), weight_decay=params[\"weight_decay\"])\n",
        "    \n",
        "elif params[\"optim\"] == \"RMSProp\":\n",
        "    gen_opt  = optim.RMSprop(gen.parameters(), lr=params[\"learning_rate\"])\n",
        "    disc_opt = optim.RMSprop(disc.parameters(), lr=params[\"learning_rate\"]*params[\"lrratio\"])\n",
        "    \n",
        "# elif optimizer == \"RMSAdam\":\n",
        "#     gen_opt  = optim.Adam(gen.parameters() , lr=learning_rate, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
        "#     disc_opt = optim.RMSprop(disc.parameters(), lr=learning_rate)    \n",
        "#     reg_gen_opt = optim.Adam(gen.parameters(), lr=learning_rate_reg, betas=(0.9,0.999))\n",
        "#     reg_disc_opt = optim.RMSprop(disc.parameters(), lr=learning_rate_reg)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcxAS0pnk6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import MultiStepLR,StepLR,CyclicLR,CosineAnnealingLR\n",
        "if params[\"LRtype\"]==\"Cyclic\":\n",
        "    scheduler_gen = CyclicLR(gen_opt, params[\"base_lr\"],params[\"learning_rate\"],\n",
        "                         step_size_up=params[\"stepsize_lr\"],\n",
        "                         step_size_down=params[\"stepsize_lr_down\"],\n",
        "                         cycle_momentum=False,mode=\"exp_range\",gamma = params[\"LRgamma\"])\n",
        "    scheduler_disc = CyclicLR(disc_opt, params[\"base_lr\"],params[\"learning_rate\"],\n",
        "                         step_size_up=params[\"stepsize_lr\"],\n",
        "                         step_size_down=params[\"stepsize_lr_down\"],\n",
        "                         cycle_momentum=False,mode=\"exp_range\",gamma = params[\"LRgamma\"])\n",
        "elif params[\"LRtype\"]==\"MStep\":\n",
        "    scheduler_gen = MultiStepLR(gen_opt, milestones=params[\"milestones\"], gamma=params[\"LRgamma\"])\n",
        "    scheduler_disc = MultiStepLR(disc_opt, milestones=params[\"milestones\"], gamma=params[\"LRgamma\"])\n",
        "elif params[\"LRtype\"]==\"Step\":\n",
        "    scheduler_gen = StepLR(gen_opt,step_size=params[\"stepsize_lr\"],gamma=params[\"LRgamma\"])\n",
        "    scheduler_disc = StepLR(disc_opt,step_size=params[\"stepsize_lr\"],gamma=params[\"LRgamma\"])\n",
        "elif params[\"LRtype\"]==\"CosA\":\n",
        "    scheduler_gen = CosineAnnealingLR(gen_opt,T_max=params[\"stepsize_lr\"],eta_min=params[\"base_lr\"])\n",
        "    scheduler_disc = CosineAnnealingLR(disc_opt,T_max=params[\"stepsize_lr\"],eta_min=params[\"base_lr\"])\n",
        "elif params[\"LRtype\"]==\"CosExp\":\n",
        "    scheduler_gen = CosineExpLR(gen_opt,T_max=params[\"stepsize_lr\"],eta_min=params[\"base_lr\"],gamma = params[\"LRgamma\"])\n",
        "    scheduler_disc = CosineExpLR(disc_opt,T_max=params[\"stepsize_lr\"],eta_min=params[\"base_lr\"],gamma = params[\"LRgamma\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBbUF9DmDL2",
        "colab_type": "text"
      },
      "source": [
        "## Load scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9v-IZO8mDL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prd_score import compute_prd, compute_prd_from_embedding, _prd_to_f_beta\n",
        "from sklearn.metrics import auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7z0U53UNIV9",
        "colab_type": "code",
        "outputId": "74bb69fb-b8f6-46a3-f898-ec67b97d884d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from Regressor import Regressor, load_embedder\n",
        "embedder = load_embedder('./embedder.tp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jumtrlNo2Gq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def D_train(PMResponse_b,ibatch,Condition_b=None,gen_input=None):\n",
        "    #=======================Train the discriminator=======================#\n",
        "    disc_loss = 0\n",
        "    disc_real = 0\n",
        "    disc_fake = 0\n",
        "    \n",
        "    # disc_real = raw_real.mean()\n",
        "    if gen_input is not None:\n",
        "        PMResponse_gen = gen_input\n",
        "    else:\n",
        "        noise = torch.randn(PMResponse_b.shape[0], NOISEIMAGE_DIM, NOISEIMAGE_DIM).to(device)\n",
        "        PMResponse_gen = gen(noise,Condition_b).to(device)\n",
        "    \n",
        "    PMResponse_b   = add_instance_noise(PMResponse_b,params[\"noiselevel\"])\n",
        "    PMResponse_gen = add_instance_noise(PMResponse_gen,params[\"noiselevel\"])\n",
        "    if params[\"AUX\"]:\n",
        "        raw_real,raw_aux_real = disc(PMResponse_b)\n",
        "        raw_fake,raw_aux_fake = disc(PMResponse_gen)\n",
        "    else:\n",
        "        raw_real,raw_aux_real = disc(PMResponse_b,Condition_b)\n",
        "        raw_fake,raw_aux_fake = disc(PMResponse_gen,Condition_b)\n",
        "    if params[\"task\"]==\"HINGE\":\n",
        "        # disc_real = -torch.min(raw_real - 1, torch.zeros(raw_real.shape[0]).to(device)).mean()\n",
        "        # disc_fake = -torch.min(-raw_fake - 1, torch.zeros(raw_fake.shape[0]).to(device)).mean()\n",
        "        disc_real = nn.ReLU()(1.0 - raw_real).mean()\n",
        "        disc_fake = nn.ReLU()(1.0 + raw_fake).mean()\n",
        "        disc_loss = disc_real+disc_fake\n",
        "        experiment.log_metric(\"d_real\", disc_real.data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_fake\", disc_fake.data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_raw_real\", raw_real.mean().data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_raw_fake\", raw_fake.mean().data.item(),step=ibatch)\n",
        "    elif params[\"task\"]==\"NORMAL\":\n",
        "        eps=1e-10\n",
        "        disc_real = F.logsigmoid(raw_real+eps).mean()\n",
        "        disc_fake = F.logsigmoid(1-raw_fake+eps).mean()\n",
        "        disc_loss = -disc_real-disc_fake\n",
        "        # print(disc_loss,disc_real,disc_fake)\n",
        "        experiment.log_metric(\"d_real\", disc_real.data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_fake\", disc_fake.data.item(),step=ibatch)\n",
        "    elif params[\"task\"]==\"WASSERSTEIN\":\n",
        "        disc_real = raw_real.mean()\n",
        "        disc_fake = raw_fake.mean()\n",
        "        disc_loss = disc_fake - disc_real\n",
        "        experiment.log_metric(\"d_real\", disc_real.data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_fake\", disc_fake.data.item(),step=ibatch)\n",
        "    # print(disc_loss,disc_real,disc_fake)\n",
        "    \n",
        "    if params[\"AUX\"]:\n",
        "        disc_aux_real = criterion(raw_aux_real,Condition_b)\n",
        "        disc_aux_fake = criterion(raw_aux_fake,Condition_b)\n",
        "        disc_aux = disc_aux_real + disc_aux_fake\n",
        "        experiment.log_metric(\"d_real_aux\", disc_aux_real.data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_fake_aux\", disc_aux_fake.data.item(),step=ibatch)\n",
        "        experiment.log_metric(\"d_aux\", disc_aux.data.item(),step=ibatch)\n",
        "        disc_loss += disc_aux\n",
        "    if params[\"gp\"]:\n",
        "        grad_penalty = calc_gradient_penalty(disc,\n",
        "                                            PMResponse_gen.data,\n",
        "                                            Condition_b,\n",
        "                                            PMResponse_b.data)\n",
        "        disc_loss += grad_penalty    \n",
        "        experiment.log_metric(\"grad_penalty\", grad_penalty.data.item(),step=ibatch)\n",
        "    # return\n",
        "    disc_opt.zero_grad()\n",
        "    disc_loss.backward()\n",
        "    disc_opt.step()\n",
        "    # print(disc_loss.data.item())\n",
        "    \n",
        "    experiment.log_metric(\"d_loss\", disc_loss.data.item(),step=ibatch)\n",
        "        \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHv4ORXp5BpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def G_train(PMResponse_b,ibatch,Condition_b=None):\n",
        "    #=======================Train the generator=======================#\n",
        "    \n",
        "    gen_loss = 0\n",
        "    gen_real = 0\n",
        "    gen_prior = 0\n",
        "    gen_noise = 0\n",
        "    noise = torch.randn(PMResponse_b.shape[0], NOISEIMAGE_DIM,NOISEIMAGE_DIM).to(device)\n",
        "    PMResponse_gen = gen(noise,Condition_b)\n",
        "    PMResponse_gen = add_instance_noise(PMResponse_gen,params[\"noiselevel\"])\n",
        "    if params[\"AUX\"]:\n",
        "        raw_gen,raw_gen_aux = disc(PMResponse_gen)\n",
        "    else:\n",
        "        raw_gen,raw_gen_aux = disc(PMResponse_gen,Condition_b)\n",
        "    if params[\"task\"]==\"HINGE\" or params[\"task\"]==\"WASSERSTEIN\":\n",
        "        gen_real = raw_gen.mean()\n",
        "        gen_loss = -gen_real\n",
        "        experiment.log_metric(\"g_real_loss\", gen_real.data.item(),step=ibatch)\n",
        "    elif params[\"task\"]==\"NORMAL\":\n",
        "        eps=1e-10\n",
        "        logp_gen_is_real = F.logsigmoid(raw_gen+eps).mean()\n",
        "        gen_loss = -logp_gen_is_real\n",
        "\n",
        "    if params[\"AUX\"]:\n",
        "        disc_fake_aux = criterion(raw_gen_aux,Condition_b)\n",
        "        experiment.log_metric(\"g_fake_aux\", disc_fake_aux.data.item(),step=ibatch)\n",
        "        gen_loss += disc_fake_aux\n",
        "    experiment.log_metric(\"g_loss\", gen_loss.data.item(),step=ibatch)\n",
        "    \n",
        "    gen_opt.zero_grad()         \n",
        "    gen_loss.backward()\n",
        "    gen_opt.step()\n",
        "    # disc.load(disc_back)\n",
        "    return PMResponse_gen\n",
        "    # gen_opt.zero_grad()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Dbdy2y-0Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CalculatePRDScore():\n",
        "    batch = 0\n",
        "    plt.figure(figsize=(15,12))\n",
        "    prd_aucs = []\n",
        "    for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b, ParticlePDG_b in validation_loader:\n",
        "        noise = torch.randn(len(EnergyDeposit_b), NOISEIMAGE_DIM, NOISEIMAGE_DIM).to(device)\n",
        "        ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)\n",
        "        ParticleMomentum_ParticlePoint_ParticlePDG_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device),ParticlePDG_b.to(device)], dim=1)\n",
        "        if params[\"Ncond_dim\"]==6:\n",
        "            EnergyDeposit_gen = gen(noise, ParticleMomentum_ParticlePoint_ParticlePDG_b)\n",
        "        elif params[\"Ncond_dim\"]==5:\n",
        "            EnergyDeposit_gen = gen(noise, ParticleMomentum_ParticlePoint_b)\n",
        "        elif params[\"Ncond_dim\"]==2:\n",
        "            EnergyDeposit_gen = gen(noise, ParticlePoint_b.to(device))\n",
        "        else:\n",
        "            EnergyDeposit_gen = gen(noise)\n",
        "\n",
        "        data_real = embedder.get_encoding(torch.tensor(EnergyDeposit_b).float().view(-1, 1, 30, 30)).detach().numpy()\n",
        "        data_fake = embedder.get_encoding(torch.tensor(EnergyDeposit_gen.cpu()).float().view(-1, 1, 30, 30)).detach().numpy()\n",
        "        precision, recall = compute_prd_from_embedding(data_real, data_fake,num_clusters=20,num_runs=10)\n",
        "        # precisions, recalls = calc_pr_rec(data_real,data_fake)\n",
        "        prd_auc_ = auc(precision, recall)\n",
        "        prd_aucs.append(prd_auc_)\n",
        "        plt.step(recall, precision, color='r', alpha=1,  label='PR')\n",
        "        # if prd_auc_>0:\n",
        "            \n",
        "        # break\n",
        "        # if batch>3:\n",
        "        #     experiment.log_figure(figure=plt)\n",
        "        #     break\n",
        "        # else:\n",
        "        #     batch+=1\n",
        "    np_prd_aucs = np.array(prd_aucs)\n",
        "    prd_auc_ = np.mean(np_prd_aucs)\n",
        "    # experiment.log_figure(figure=plt)\n",
        "    return prd_auc_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEDWICNr93MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def SaveModels(disc,gen,epoch):\n",
        "    \n",
        "#     # gen  = ModelGConvTranspose_SMALL(z_dim=NOISEIMAGE_DIM, MomentumPointPDGScale = MomentumPointPDGScale.to(device),EnergyScale = EnergyDepositScale.to(device), Nredconv_gen=Nredconv_gen).to(device)\n",
        "#     use_bn = True\n",
        "#     if TASK=='WASSERSTEIN' or TASK=='CRAMER':\n",
        "#         use_bn = False\n",
        "        \n",
        "#     disc_cpu = ModelD(\n",
        "#         cond_dim=6, \n",
        "#         MomentumPointPDGScale=MomentumPointPDGScale.to(device), \n",
        "#         MomentumPointPDGOffset=MomentumPointPDGOffset.to(device), \n",
        "#         EnergyScale = EnergyDepositScale.to(device),\n",
        "#         EnergyOffset = EnergyDepositOffset.to(device),\n",
        "#         Nredconv_dis=Nresblock,\n",
        "#         dropout_fraction=dropout_disc,\n",
        "#         use_bn=use_bn,\n",
        "#         use_additionalinfo = use_additionalinfo\n",
        "#         )\n",
        "    \n",
        "#     disc_cpu.load_state_dict(disc.state_dict())\n",
        "#     # generator_cpu.eval()\n",
        "#     torch.save(disc_cpu.state_dict(), GetDiscPath(epoch))\n",
        "#     gen_cpu  = ModelGConvTranspose(\n",
        "#         z_dim=NOISEIMAGE_DIM, \n",
        "#         MomentumPointPDGScale = MomentumPointPDGScale.to(device),\n",
        "#         MomentumPointPDGOffset=MomentumPointPDGOffset.to(device), \n",
        "#         EnergyScale = EnergyDepositScale.to(device), \n",
        "#         EnergyOffset = EnergyDepositOffset.to(device),\n",
        "#         Nredconv_gen=Nredconv_gen,\n",
        "#         add_dense = add_dense,\n",
        "#         use_resblock =use_resblock\n",
        "#     )\n",
        "#     gen_cpu.load_state_dict(gen.state_dict())\n",
        "#     # generator_cpu.eval()\n",
        "#     torch.save(gen_cpu.state_dict(), GetGenPath(epoch))\n",
        "#     return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drHHyzotaBEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Training(EnergyDeposit_b,ibatch,Condition_b=None):\n",
        "    for itrain in range(params[\"ntrain_d\"]):\n",
        "            D_train(EnergyDeposit_b,ibatch, Condition_b)\n",
        "    EnergyDeposit_gen = G_train(EnergyDeposit_b,ibatch, Condition_b)\n",
        "    return EnergyDeposit_gen\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c18oWujomDL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(epochs):\n",
        "    ibatch = 0 \n",
        "    # prd_auc = []  \n",
        "    for epoch in tqdm(range(restartfrom,epochs)):\n",
        "        first = True\n",
        "        experiment.log_metric(\"learning_rate\", scheduler_gen.get_lr(),step=epoch)\n",
        "        if params[\"test_sample\"]:\n",
        "            for i in range(int(params[\"train_size\"]/params[\"batch_size\"])):\n",
        "                EnergyDeposit_b = Make_Sample(params[\"batch_size\"],30,30).to(device)\n",
        "                EnergyDeposit_gen = Training(EnergyDeposit_b,ibatch)\n",
        "                ibatch += 1\n",
        "        else:\n",
        "            for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b, ParticlePDG_b in train_loader:\n",
        "                EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b, ParticlePDG_b = EnergyDeposit_b.to(device), \\\n",
        "                                                                    ParticleMomentum_b.to(device), \\\n",
        "                                                                    ParticlePoint_b.to(device),\\\n",
        "                                                                    ParticlePDG_b.view(-1,1).to(device)\n",
        "\n",
        "                # ParticlePoint_b = ParticlePoint_b.to(device)], dim=1)\n",
        "                ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)\n",
        "                ParticleMomentum_ParticlePoint_ParticlePDG_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device), ParticlePDG_b.to(device)], dim=1)\n",
        "                \n",
        "                if params[\"conditional\"]:\n",
        "                    if params[\"Ncond_dim\"]==6:\n",
        "                        Condition_b = ParticleMomentum_ParticlePoint_ParticlePDG_b\n",
        "                    elif params[\"Ncond_dim\"]==5:\n",
        "                        Condition_b = ParticleMomentum_ParticlePoint_b\n",
        "                    elif params[\"Ncond_dim\"]==2:\n",
        "                        Condition_b = ParticlePoint_b\n",
        "                else:\n",
        "                    Condition_b = None\n",
        "                EnergyDeposit_gen = Training(EnergyDeposit_b,ibatch,Condition_b)\n",
        "                ibatch += 1\n",
        "            # for itrain in range(params[\"ntrain_d\"]):\n",
        "            #     D_train(EnergyDeposit_b,ibatch, Condition_b)\n",
        "            # EnergyDeposit_gen = G_train(EnergyDeposit_b,ibatch, Condition_b)\n",
        "            # if LIPSITZ_WEIGHTS:                    \n",
        "            #     [p.data.clamp_(-0.01, 0.01) for p in disc.parameters()]\n",
        "            \n",
        "            # if first:\n",
        "            # if ibatch %300 ==1:\n",
        "        if not params[\"test_sample\"]:\n",
        "            prd_auc_ = CalculatePRDScore()\n",
        "            # prd_auc.append(prd_auc_)\n",
        "            experiment.log_metric(\"PRD_AUC\", prd_auc_,step=epoch)\n",
        "        # SaveModels(disc,gen,epoch)\n",
        "        # first=False\n",
        "    \n",
        "        clear_output()\n",
        "    # if ibatch %50 ==2:\n",
        "        plt.figure(figsize=(15,12))\n",
        "        grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\n",
        "        for event in range(3):\n",
        "            plt.subplot(grid[0,event])\n",
        "            plt.imshow(EnergyDeposit_b[event][0].cpu().detach().numpy(),vmin=0,origin='lower')\n",
        "            plt.colorbar()\n",
        "            # plt.title(\"Momentum: (%f,%f,%f) \\n Position: (%f,%f)\"%(\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][0],\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][1],\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][2],\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][3],\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][4]\n",
        "            #     )\n",
        "            # )\n",
        "            # plt.title(\"True: %f, Rec: %f\"%(\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][0],\n",
        "            #     ParticleMomentum_ParticlePoint_ParticlePDG_b[event][0],\n",
        "            #     )\n",
        "            # )\n",
        "            plt.subplot(grid[1, event])\n",
        "            plt.imshow(EnergyDeposit_gen[event][0].cpu().detach().numpy(),vmin=0,origin='lower')\n",
        "            plt.colorbar()\n",
        "                \n",
        "        experiment.log_figure(figure_name=\"status%d\"%(epoch),figure=plt,step =epoch)\n",
        "        # time.sleep(0.5)\n",
        "        plt.close()\n",
        "#             plt.show()\n",
        "        # ibatch += 1\n",
        "        scheduler_gen.step()\n",
        "        scheduler_disc.step()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "zPLcnlKhmDMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with experiment.train():\n",
        "    run_training(params[\"Nepoch\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2fiNDMmDMJ",
        "colab_type": "text"
      },
      "source": [
        "#### Transfer generator on CPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTumKz8smDMK",
        "colab_type": "code",
        "outputId": "c6b7a378-ebfd-429f-a838-0001795e6631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# generator_cpu = ModelGConvTranspose(z_dim=NOISEIMAGE_DIM,MomentumPointPDGScale = MomentumPointPDGScale,EnergyScale = EnergyDepositScale)\n",
        "generator_cpu = Generator(params[\"dropout_gen\"],params[\"Ncond_dim\"])\n",
        "generator_cpu.load_state_dict(gen.state_dict())\n",
        "# generator_cpu.load_state_dict(torch.load('/content/drive/My Drive/gan_wgan.pt'))\n",
        "# generator_cpu.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrzMuZGzmDMM",
        "colab_type": "text"
      },
      "source": [
        "### Save model on disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXwtnaSBmDMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(generator_cpu.state_dict(), '/content/drive/My Drive/gan_w.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nITR2bZwmDMO",
        "colab_type": "text"
      },
      "source": [
        "## Making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihAqZQxvmDMP",
        "colab_type": "text"
      },
      "source": [
        "#### Validation predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rVhVrubhmDMP",
        "colab_type": "code",
        "outputId": "642542a1-405d-4993-818e-907263ab0ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "data_val = np.load(val_data_path, allow_pickle=True)\n",
        "ParticleMomentum_val = torch.tensor(data_val['ParticleMomentum']).float()\n",
        "ParticlePoint_val = torch.tensor(data_val['ParticlePoint'][:, :2]).float()\n",
        "ParticlePDG_val = torch.tensor(data_val['ParticlePDG'].reshape(-1,1)).float()\n",
        "ParticleMomentum_ParticlePoint_ParticlePDG_val = torch.cat([ParticleMomentum_val, ParticlePoint_val,ParticlePDG_val], dim=1)\n",
        "calo_dataset_val = utils.TensorDataset(ParticleMomentum_ParticlePoint_ParticlePDG_val)\n",
        "calo_dataloader_val = torch.utils.data.DataLoader(calo_dataset_val, batch_size=1024, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    EnergyDeposit_val = []\n",
        "    for ParticleMomentum_ParticlePoint_ParticlePDG_val_batch in tqdm(calo_dataloader_val):\n",
        "        noise = torch.randn(len(ParticleMomentum_ParticlePoint_ParticlePDG_val_batch[0]), NOISEIMAGE_DIM,NOISEIMAGE_DIM)\n",
        "        EnergyDeposit_val_batch = generator_cpu(noise, ParticleMomentum_ParticlePoint_ParticlePDG_val_batch[0]).detach().numpy()\n",
        "        EnergyDeposit_val.append(EnergyDeposit_val_batch)\n",
        "    np.savez_compressed('./data_val_prediction.npz', \n",
        "                        EnergyDeposit=np.concatenate(EnergyDeposit_val, axis=0).reshape(-1, 30, 30))\n",
        "\n",
        "    del EnergyDeposit_val\n",
        "del data_val; del ParticleMomentum_val; del ParticlePoint_val; del ParticleMomentum_ParticlePoint_ParticlePDG_val;\n",
        "del calo_dataset_val; calo_dataloader_val"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-bbbbdcc84af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mParticleMomentum_ParticlePoint_ParticlePDG_val_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalo_dataloader_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParticleMomentum_ParticlePoint_ParticlePDG_val_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOISEIMAGE_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNOISEIMAGE_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mEnergyDeposit_val_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParticleMomentum_ParticlePoint_ParticlePDG_val_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mEnergyDeposit_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnergyDeposit_val_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     np.savez_compressed('./data_val_prediction.npz', \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-10a5276ed30b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, condition)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# condition = (condition-MomentumPointPDGOffset)/MomentumPointPDGScale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1024 x 31], m2: [30 x 256] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz2T9U3-mDMS",
        "colab_type": "text"
      },
      "source": [
        "#### Test predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "666U1Q_smDMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = np.load(test_data_path, allow_pickle=True)\n",
        "ParticleMomentum_test = torch.tensor(data_test['ParticleMomentum']).float()\n",
        "ParticlePoint_test = torch.tensor(data_test['ParticlePoint'][:, :2]).float()\n",
        "ParticlePDG_test = torch.tensor(data_test['ParticlePDG'].reshape(-1,1)).float()\n",
        "ParticleMomentum_ParticlePoint_ParticlePDG_test = torch.cat([ParticleMomentum_test, ParticlePoint_test, ParticlePDG_test], dim=1)\n",
        "calo_dataset_test = utils.TensorDataset(ParticleMomentum_ParticlePoint_ParticlePDG_test)\n",
        "calo_dataloader_test = torch.utils.data.DataLoader(calo_dataset_test, batch_size=1024, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    EnergyDeposit_test = []\n",
        "    for ParticleMomentum_ParticlePoint_ParticlePDG_test_batch in tqdm(calo_dataloader_test):\n",
        "        noise = torch.randn(len(ParticleMomentum_ParticlePoint_ParticlePDG_test_batch[0]), NOISE_DIM)\n",
        "        EnergyDeposit_test_batch = generator_cpu(noise, ParticleMomentum_ParticlePoint_ParticlePDG_test_batch[0]).detach().numpy()\n",
        "        EnergyDeposit_test.append(EnergyDeposit_test_batch)\n",
        "    np.savez_compressed('./data_test_prediction.npz', \n",
        "                        EnergyDeposit=np.concatenate(EnergyDeposit_test, axis=0).reshape(-1, 30, 30))\n",
        "\n",
        "    del EnergyDeposit_test\n",
        "del data_test; del ParticleMomentum_test; del ParticlePoint_test; del ParticleMomentum_ParticlePoint_ParticlePDG_test;\n",
        "del calo_dataset_test; calo_dataloader_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIKCQMtmDMU",
        "colab_type": "text"
      },
      "source": [
        "## `zip-zip` files together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvbj0nhzmDMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip solution.zip data_val_prediction.npz data_test_prediction.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG-4EvGOmDMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink('./solution.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGp0K2LrmDMY",
        "colab_type": "text"
      },
      "source": [
        "# A few words about metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_j_38JamDMa",
        "colab_type": "text"
      },
      "source": [
        "### Lets generate some fake data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47UHZk7B0Lei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b, ParticlePDG_b in validation_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kyqaIyhmDMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = torch.randn(len(ParticleMomentum_b), NOISE_DIM)\n",
        "with torch.no_grad():\n",
        "    ParticleMomentum_ParticlePoint_ParticlePDG = torch.cat([ParticleMomentum_b, \n",
        "                                                ParticlePoint_b,\n",
        "                                                ParticlePDG_b], dim=1)\n",
        "    EnergyDeposit_gen = generator_cpu(noise, ParticleMomentum_ParticlePoint_ParticlePDG.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s353zPylmDMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EnergyDeposit_gen = EnergyDeposit_gen.detach().cpu().numpy().reshape(-1, 30, 30)\n",
        "EnergyDeposit_b = EnergyDeposit_b.detach().cpu().numpy().reshape(-1, 30, 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TD90ylsmDMd",
        "colab_type": "text"
      },
      "source": [
        "#### Plot one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxhjGZGYmDMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(EnergyDeposit_gen[2])\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTftFOhSmDMg",
        "colab_type": "text"
      },
      "source": [
        "## Calculate PRD score between these batch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWEusQu0mDMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.batchnorm0 = nn.BatchNorm2d(1)\n",
        "        self.conv1 = nn.Conv2d(1, 16, 2, stride=2)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 2, stride=2)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 2, stride=2)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 2)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        \n",
        "        self.fc1 = nn.Linear(256, 256) \n",
        "        self.batchnorm4 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 2 + 3)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.batchnorm0(self.dropout(x))\n",
        "        x = self.batchnorm1(self.dropout(F.relu(self.conv1(x))))\n",
        "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
        "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x)) # 64, 5, 5\n",
        "        x = x.view(len(x), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.batchnorm4(self.dropout(F.relu(self.fc1(x))))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        return self.fc4(x), self.fc5(x)\n",
        "    \n",
        "    def get_encoding(self, x):\n",
        "        x = self.batchnorm0(self.dropout(x))\n",
        "        x = self.batchnorm1(self.dropout(F.relu(self.conv1(x))))\n",
        "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
        "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x)) # 64, 5, 5\n",
        "        x = x.view(len(x), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.batchnorm4(self.dropout(F.relu(self.fc1(x))))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "def load_embedder(path):\n",
        "    embedder = torch.load(path)\n",
        "    embedder.eval()\n",
        "    return embedder\n",
        "\n",
        "embedder = load_embedder('./embedder.tp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd63IOwVmDMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_real = embedder.get_encoding(torch.tensor(EnergyDeposit_b).float().view(-1, 1, 30, 30)).detach().numpy()\n",
        "data_fake = embedder.get_encoding(torch.tensor(EnergyDeposit_gen).float().view(-1, 1, 30, 30)).detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGM2uI_qmDMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pr_aucs(precisions, recalls):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    pr_aucs = []\n",
        "    for i in range(len(recalls)):\n",
        "        plt.step(recalls[i], precisions[i], color='b', alpha=0.2,  label='PR-AUC={}'.format(auc(precisions[i], recalls[i])))\n",
        "        pr_aucs.append(auc(precisions[i], recalls[i]))\n",
        "    plt.step(np.mean(recalls, axis=0), np.mean(precisions, axis=0), color='r', alpha=1,  label='average')\n",
        "    plt.fill_between(np.mean(recalls, axis=0), \n",
        "                     np.mean(precisions, axis=0) - np.std(precisions, axis=0) * 3,\n",
        "                     np.mean(precisions, axis=0) + np.std(precisions, axis=0) * 3, color='g', alpha=0.2,  label='std')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "\n",
        "    # plt.ylim([0.0, 1.05])\n",
        "    # plt.xlim([0.0, 1.0])\n",
        "    print(np.mean(pr_aucs), np.std(pr_aucs))\n",
        "    plt.legend()\n",
        "    \n",
        "    return pr_aucs\n",
        "\n",
        "def calc_pr_rec(data_real, data_fake, num_clusters=20, num_runs=10, NUM_RUNS=10):\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for i in tqdm(range(NUM_RUNS)):\n",
        "        precision, recall = compute_prd_from_embedding(data_real, data_fake, num_clusters=num_clusters, num_runs=num_runs)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    return precisions, recalls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NApjdKVmDMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precisions, recalls = calc_pr_rec(data_real, data_fake, num_clusters=100, num_runs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0D4-j__mDMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_aucs = plot_pr_aucs(precisions, recalls)\n",
        "plt.title('Num_clusters={}, num_runs={}, first third'.format(100, 20))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erF_MpJ_mDMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_aucs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkiQuGlmDMs",
        "colab_type": "text"
      },
      "source": [
        "## Physical metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI_33EaxmDMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.lines as mlines\n",
        "def newline(p1, p2):\n",
        "    ax = plt.gca()\n",
        "    xmin, xmax = ax.get_xbound()\n",
        "\n",
        "    if(p2[0] == p1[0]):\n",
        "        xmin = xmax = p1[0]\n",
        "        ymin, ymax = ax.get_ybound()\n",
        "    else:\n",
        "        ymax = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmax-p1[0])\n",
        "        ymin = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmin-p1[0])\n",
        "\n",
        "    l = mlines.Line2D([xmin,xmax], [ymin,ymax])\n",
        "    ax.add_line(l)\n",
        "    return l\n",
        "\n",
        "def plot_axes_for_shower(ecal, point, p):\n",
        "    x = np.linspace(-14.5, 14.5, 30)\n",
        "    y = np.linspace(-14.5, 14.5, 30)\n",
        "\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    zoff = 25.\n",
        "    ipic = 3\n",
        "    orth = np.array([-p[1], p[0]])\n",
        "\n",
        "    pref = point[:2] + p[:2] * zoff / p[2]\n",
        "\n",
        "    p1 = pref - 10 * p[:2]\n",
        "    p2 = pref + 10 * p[:2]\n",
        "    p3 = pref - 10 * orth\n",
        "    p4 = pref + 10 * orth\n",
        "\n",
        "    plt.contourf(xx, yy, np.log(ecal + 1), cmap=plt.cm.inferno)\n",
        "    newline(p1, p2)\n",
        "    newline(p3, p4)\n",
        "    plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR08nJ9gmDMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 2\n",
        "plot_axes_for_shower(EnergyDeposit[idx], point=ParticlePoint[idx].detach().numpy(),\n",
        "                     p=ParticleMomentum[idx].detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUCqH0_SmDMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from calogan_metrics import get_assymetry, get_shower_width, get_sparsity_level"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbvzfsYUmDM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assym = get_assymetry(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=False)\n",
        "assym_ortho = get_assymetry(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=True)\n",
        "sh_width = get_shower_width(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=False)\n",
        "sh_width_ortho = get_shower_width(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=True)\n",
        "sparsity_level = get_sparsity_level(EnergyDeposit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eJLvn3EmDM2",
        "colab_type": "text"
      },
      "source": [
        "## Longitudual cluster asymmetry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEORbnE7mDM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(font_scale=2)\n",
        "plt.hist(assym, bins=50, range=[-1, 1], color='red', alpha=0.3, normed=True, label='MC');\n",
        "plt.xlabel('Longitudual cluster asymmetry')\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15wFghYcmDM3",
        "colab_type": "text"
      },
      "source": [
        "## Transverse cluster asymmetry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfge5LTMmDM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(font_scale=2)\n",
        "plt.hist(assym_ortho, bins=50, range=[-1, 1], color='red', alpha=0.3, normed=True, label='MC');\n",
        "plt.xlabel('Transverse cluster asymmetry')\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4lWh8VAmDM5",
        "colab_type": "text"
      },
      "source": [
        "## Cluster longitudual width"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB27rAtImDM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(sh_width, bins=50, range=[0, 15], normed=True, alpha=0.3, color='red', label='MC');\n",
        "plt.title('Shower longitudial width')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('Cluster longitudual width [cm]')\n",
        "plt.ylabel('Arbitrary units')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC-HiFBamDM7",
        "colab_type": "text"
      },
      "source": [
        "## Cluster trasverse width"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBIu8gwLmDM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(sh_width_ortho, bins=50, range=[0,10], normed=True, alpha=0.3, color='blue', label='MC');\n",
        "#plt.title('Shower transverse width')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('Cluster trasverse width [cm]')\n",
        "plt.ylabel('Arbitrary units')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pFbBYvOmDNA",
        "colab_type": "text"
      },
      "source": [
        "## Sparsity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob53fPO_mDNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphas = np.log(np.logspace(-5, -1, 20))\n",
        "means_r = np.mean(sparsity_level, axis=1)\n",
        "stddev_r = np.std(sparsity_level, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QygKaRldmDNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(alphas, means_r, color='red')\n",
        "plt.fill_between(alphas, means_r-stddev_r, means_r+stddev_r, color='red', alpha=0.3)\n",
        "plt.legend(['MC'])\n",
        "plt.title('Sparsity')\n",
        "plt.xlabel('log10(Threshold/GeV)')\n",
        "plt.ylabel('Fraction of cells above threshold')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-imUBMEmDND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from calogan_metrics import get_physical_stats\n",
        "real_phys_stats = get_physical_stats(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy())\n",
        "gen_phys_stats = get_physical_stats(EnergyDeposit_gen, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp_s9vGcmDNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precisions, recalls = calc_pr_rec(real_phys_stats, gen_phys_stats, num_clusters=100, num_runs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btvs99JKmDNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_aucs = plot_pr_aucs(precisions, recalls)\n",
        "plt.title('Num_clusters={}, num_runs={}, first third'.format(100, 20))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOyBgN-4mDNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_aucs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}